---
title: "Proyecto"
author: "Diego Franco, Daniel Gonzalez, Angel Carrera"
format: html
editor: visual
---

```{r echo=F, include=FALSE}
library(nortest)
library(dplyr)
library(hopkins)
library(factoextra)
library(ggrepel)
library(cluster)
library(flexclust)
library(FeatureImpCluster)
library(stringr)
library(tidyr)
library(stats)
library(graphics)
library(NbClust)
library(mclust)
library(GGally)
library(corrplot)
library(caret)
library(ggplot2)
library(kableExtra)
library(e1071)
library(rpart)
library(rpart.plot)
library(naivebayes)
library(randomForest)
library(RColorBrewer)
library(ca)
library(vcd)
library(xgboost)
library(mlr)
library(glmnet)
library(nnet)
```

```{r echo=F, include=FALSE}
datos <- read.csv("defun_nac.csv")
```

# Defunciones fetales y nacimientos en Guatemala

## 1. Definición de variable respuesta

La variable por utilizar será **TIPO**, la cual indica si un parto terminó en una defunción fetal o en un nacimiento. Debido a que la investigación de este trabaja está orientada a crear una herramienta que permita ayudar a identificar partos potencialmente problemáticos, se decidió usar esta variable como la respuesta. Esto permitirá saber, dadas las condiciones de una mujer embarazada, si tiene algún riesgo.

Cabe mencionar que esta variable no existía dentro del dataset inicial, sino que a la hora de hacer la unión entre los conjuntos de datos de nacimientos y defunciones se indicó de qué conjunto de datos era proveniente cada observación.

### 1.1 Limpieza de datos

### Análsis de NA y limpieza

Los datos tienen un encoding en el que se pone un número (como 99 o 999) para indicar que no se tiene información. Por lo tanto es necesario reemplazar estos números por NA.

```{r echo=F}
datos$SEXO <- ifelse(datos$SEXO == 9, NA, datos$SEXO)
datos$SEMGES <- ifelse(datos$SEMGES == 99, NA, datos$SEMGES)
datos$EDADM <- ifelse(datos$EDADM == 999, NA, datos$EDADM)
datos$DEPREM <- ifelse(datos$DEPREM == 9999, NA, datos$DEPREM)
datos$MUPREM <- ifelse(datos$MUPREM == 9999, NA, datos$MUPREM)
datos$ESCIVM <- ifelse(datos$ESCIVM == 9, NA, datos$ESCIVM)
datos$ESCOLAM <- ifelse(datos$ESCOLAM == 9, NA, datos$ESCOLAM)
datos$SITIOOCU <- ifelse(datos$SITIOOCU == 9, NA, datos$SITIOOCU)
datos$TOHITE <- ifelse(datos$TOHITE == 99, NA, datos$TOHITE)
datos$TOHINM <- ifelse(datos$TOHINM == 99, NA, datos$TOHINM)
datos$TOHIVI <- ifelse(datos$TOHIVI == 99, NA, datos$TOHIVI)
```

Además, también se tienen muchas variables con valores numéricos pero que son cualitativas. Por lo tanto, se conviertieron en factores.

```{r echo=F}
cualitativas <- c('DEPREG', 'MUPREG', 'MESREG', 'AÑOREG', 'DEPOCU', 'MUPOCU', 'SEXO', 'DIAOCU', 'MESOCU', 'TIPAR', 'DEPREM', 'MUPREM', 'ESCIVM', 'ESCOLAM', 'ASISREC', 'SITIOOCU', 'TIPO')
datos[, cualitativas] <- lapply(datos[, cualitativas], as.factor)
```

## Eliminación de variables no significativas

Hay ciertos años que no tienen todas las variables que años mas recientes tienen entonces para tener una mayor covertura de años se eliminan las variables que no tienen datos en todos los años.

```{r}
datos <- na.omit(datos)
datos <- select(datos, -SEMGES, -MUPREG, -MUPOCU, -DEPOCU, -MUPREM, -DEPREG, -DEPREM, -TOHITE, -AÑOREG)
# colnames(datos)[2] <- 'ANOREG'
```

## 2. Obtención de datos de entrenamiento y prueba

### Creación de train y test

```{r}
porcentaje <- 0.7
set.seed(123)

corte <- sample(nrow(datos), nrow(datos) * porcentaje)
train <- datos[corte, ]
test <- datos[-corte, ]
```

Para obtener los conjuntos de entrenamiento y prueba, se utilizó un método de división aleatoria.

En este caso, se dividió el conjunto de datos en un 70% para entrenamiento y un 30% para pruebas. Esto significa que el 70% de las muestras se utilizaron para entrenar el modelo y el 30% restante se reservaron para evaluar el rendimiento del modelo en datos no vistos.

```{r}
num_vivos <- sum(train$TIPO == 'vivo')
porcentaje_vivos <- (num_vivos / nrow(train)) * 100
porcentaje_vivos
100 - porcentaje_vivos

num_vivos <- sum(test$TIPO == 'vivo')
porcentaje_vivos <- (num_vivos / nrow(test)) * 100
porcentaje_vivos
100 - porcentaje_vivos
```

Al realizar la limpieza de NAs, sucede que se desbalancea el dataset, quedando más registros vivos. Tanto en train como en test se tiene un 67% de datos de embarazos exitosos y un 33% de datos de defunciones fetales. No se tiene un desbalance tan drástico, pero se buscará usar una métrica como F1, aparte de accuracy, de forma que se penalice este desbalance y se puedan evaluar de una mejor manera los modelos.

## 3. Transformaciones realizadas para poder usar los modelos

Para poder utilizar los modelos elegidos con aterioridad, se realizaron diversas transformaciones para asegurar la calidad y la adecuación de los datos. En primer lugar, se llevó a cabo un análisis exhaustivo de los valores faltantes (NA) presentes en el conjunto de datos. Se identificaron las variables con valores faltantes y se decidió hacer la eliminación de las observaciones correspondientes. Esto permitió asegurar que los datos utilizados estuvieran completos y representativos.

Además, se llevó a cabo una evaluación de la relevancia y significancia de las variables presentes en el conjunto de datos. Mediante técnicas de análisis exploratorio, se identificaron las variables que tenían una contribución limitada o nula para los objetivos del análisis. Estas variables no significativas fueron eliminadas del conjunto de datos, simplificando así su estructura y reduciendo la complejidad.

### 3.1 Datos necesarios para SVM y XGBoost

Para la SVM y el XGBoost se necesitaran los datos codificados, por lo que se procede a codificar las variables cualitativass en cuantitativas y para esto se utiliza el método de codificación one-hot. Para la variable objetivo se utiliza la codificación de 1 para un embarazo fallido y 0 para un embarazo exitoso.

```{r}

set.seed(123)
data <- datos
cualitativas2 <- c('MESREG', 'AÑOREG', 'SEXO', 'DIAOCU', 'MESOCU', 'TIPAR', 'ESCIVM', 'ESCOLAM', 'ASISREC', 'SITIOOCU')
cualitativas2_indices <- which(colnames(data) %in% cualitativas2)
data$TIPO <- ifelse(data$TIPO == "muerto", 1, 0)

encoding_model <- dummyVars(~., data = data[, cualitativas2_indices], fullRank = TRUE)

encoded_data <- predict(encoding_model, newdata = data[, cualitativas2_indices])

final_data <- cbind(data[, !cualitativas2_indices], encoded_data)
final_data$TIPO <- data$TIPO



porcentaje <- 0.7

corte <- sample(nrow(final_data), nrow(final_data) * porcentaje)
train1 <- final_data[corte, ]
test1 <- final_data[-corte, ]

```

## 4. Creación de modelos

### 4.1 SVM

#### 4.1.1 Parameter tuning

```{r}
# Define los rangos de los parámetros que deseas ajustar
# tune.grid <- expand.grid(.cost = 10^(-1:2),   # Por ejemplo, costos de 0.1, 1, 10, 100
#                          .gamma = c(0.5, 1))  # Por ejemplo, gammas de 0.5, 1

# # Realiza el ajuste
# tune.out <- tune(svm, TIPO~., data = train, kernel = "linear",
#                  ranges = tune.grid)

# # Imprime los resultados del ajuste
# print(tune.out)

# # Obtén el mejor modelo
# best.model <- tune.out$best.model

# # Imprime el mejor modelo
# print(best.model)
```

Se obtuvo del parameter tuning que el mejor kernel es el lineal, con costo de 10 y gamma de 0.1.

#### 4.1.2 Curva de aprendizaje

```{r}
library(dplyr)
# Dividir los datos en conjuntos de entrenamiento y prueba

# modeloSVM <- svm(TIPO~., data = train, kernel = "linear", cost = 10, gamma = 0.1)
modeloSVM <- readRDS("./modelos/modeloSVM.rds")
```

```{r}
folds <- createFolds(train$TIPO, k = 25, list = TRUE, returnTrain = TRUE)
    
    rmse_train <- vector(length = length(folds))
    rmse_test <- vector(length = length(folds))

   
    for (i in 1:length(folds)){
        train_data <- train[folds[[i]],]
        test_data <- train[-folds[[i]],]

        pred_train <- predict(modeloSVM, newdata = train_data )
        pred <- predict(modeloSVM, newdata = test_data)

        rmse_train[i] <- confusionMatrix(pred_train, train_data$TIPO)$overall["Accuracy"]
        rmse_test[i] <- confusionMatrix(pred, test_data$TIPO)$overall["Accuracy"]
    }

    rmse_df <- data.frame(
  Fold = 1:length(folds),
  RMSE_Train = rmse_train,
  RMSE_Test = rmse_test
)

# Crear el gráfico de línea
ggplot(rmse_df, aes(x = Fold)) +
  geom_line(aes(y = RMSE_Train, color = "Conjunto de Entrenamiento")) +
  geom_line(aes(y = RMSE_Test, color = "Conjunto de Prueba")) +
  labs(title = "Valores de RMSE en la validación cruzada",
       x = "Fold",
       y = "RMSE") +
  scale_color_manual(values = c("Conjunto de Entrenamiento" = "blue", "Conjunto de Prueba" = "red")) +
  theme_minimal()
```

Se puede observar un gráfico en donde los datos no son representativos, esto porque las líneas se cruzan entre sí y no hay una relación clara entre los datos de entrenamiento y prueba.

#### 4.1.3 Predicciones

```{r}
print(modeloSVM)

svmpredicciones <- predict(modeloSVM, test)
svmConfusionMatrix <- confusionMatrix(svmpredicciones, test$TIPO)
print(svmConfusionMatrix)
```

```{r}
f1_score <- svmConfusionMatrix$byClass["F1"]
f1_score
```

### 4.2 Red neuronal

#### 4.2.1 Parameter tuning red neuronal

```{r}
param_grid <- expand.grid(size = c(5, 10, 15), decay = c(0.1, 0.01, 0.001))


ctrl <- trainControl(method = "cv", number = 5)  # Cross-validation control
tuned_model <- caret::train(TIPO ~ ., data = train, method = "nnet",
                     tuneGrid = param_grid, trControl = ctrl, metric = "Accuracy", trace=FALSE)

best_size <- tuned_model$bestTune$size
best_decay <- tuned_model$bestTune$decay
print(best_size)
print(best_decay)

```

El tuneo de parámetros se realizó con una sola capa. Se usaron topologías de 5, 10 y 15 neuronas. Aparte, se evaluó un decay de 0.1, 0.01 y 0.001. Luego de ejecutar el procedimiento, se obtuvo que el mejor modelo usa 10 neuronas y un decay de 0.01. Sin embargo, este modelo dió overfitting. Cuando se usó 0.0001 de decay y 5 neuronas, no hubo overfitting.

#### 4.2.3 Creación de curva de aprendizaje

```{r}
datos.task = makeClassifTask(data = train, target = "TIPO")
rin2 = makeResampleDesc(method = "CV", iters = 10, predict = "both")
lrn = makeLearner("classif.nnet", size = 5, decay = 0.0001, maxit = 1000, trace = FALSE)
lc2 = generateLearningCurveData(learners = lrn, task = datos.task,
                                    percs = seq(0.1, 1, by = 0.1),
                                    measures = list(ber, setAggregation(ber, train.mean)), resampling = rin2,
                                    show.info = FALSE)
plotLearningCurve(lc2, facet = "learner")
```

Como se puede observar en la gráfica, la curva de test y la curva de train, tienen un comportamiento muy similar. A pesar de que hay diversos aumentos y disminuciones en las curvas, estas son muy cercanas. En conclusión, estas convergen, por lo que no se da overfitting.

#### 4.2.4 Predicción y datos de validación

```{r}
porcentaje <- 0.5

nnet_model <- nnet(TIPO ~ ., data = train, size=5, decay=0.0001, softmax = FALSE)

# Make predictions on the test set
predictions1 <- as.factor(predict(nnet_model, newdata = test, type = "class"))

confusion_matrix <- confusionMatrix(reference = test$TIPO, data = predictions1)

confusion_matrix
```

```{r}
porcentaje <- 0.5
corte <- sample(nrow(test), nrow(test) * porcentaje)
test2 <- test[corte, ]
validation2 <- test[-corte,]

nnet_model <- nnet(TIPO ~ ., data = train, size=5, decay=0.0001, softmax = FALSE)

# Make predictions on the test set
predictions1 <- as.factor(predict(nnet_model, newdata = test2, type = "class"))
predictions2 <- as.factor(predict(nnet_model, newdata = validation2, type = "class"))

confusion_matrix <- confusionMatrix(reference = test2$TIPO, data = predictions1)
confusion_matrix2 <- confusionMatrix(reference = validation2$TIPO, data = predictions2)

confusion_matrix
confusion_matrix2
```

```{r}
f1_score <- confusion_matrix$byClass["F1"]
f1_score

f1_score <- confusion_matrix2$byClass["F1"]
f1_score
```

Se usó un set de datos de prueba y uno validación para poder evaluar el overfitting. Las métricas entre el set de validación y el de prueba no difieren significativamente, por lo que se puede confirmar que no hay overfitting.

### 4.3 XGBoost

#### 4.3.1 Parameter tunning

```{r}
set.seed(123)
TIPO_temp <- train1$TIPO
train1$TIPO <- train$TIPO
task <- makeClassifTask(data = train1, target = "TIPO")
param_grid <- makeParamSet(
  makeDiscreteParam("nrounds", values = seq(100, 500, by = 20)),
  makeIntegerParam("max_depth", lower = 3, upper = 10),
  makeNumericParam("eta", lower = 0.01, upper = 0.3)
)
control_tune <- makeTuneControlRandom(maxit = 30)
resampling <- makeResampleDesc("CV", iters = 5)
measure <- acc
tuned_model <- tuneParams(
  learner = "classif.xgboost",  # For classification, use "classif.xgboost"; for regression, use "regr.xgboost"
  task = task,
  resampling = resampling,
  measures = measure,
  par.set = param_grid,
  control = control_tune,
  show.info = TRUE
)

best_params <- tuned_model$x
best_params
```

Despues de realizar el parameter tunning se obtuvo que los mejores parametros son: nrounds = 500, max_depth = 4, eta = 0.2254931. Esto se debe a que se obtuvo el mejor accuracy con estos parametros.

#### 4.3.2 Curva de aprendizaje

```{r}
train1$TIPO <- train$TIPO
datos.task <- makeClassifTask(data = train1, target = "TIPO")
rin2 <- makeResampleDesc(method = "CV", iters = 10, predict = "both")
lrn <- makeLearner("classif.xgboost", nrounds = 500, max_depth = 4, eta = 0.2254931)
lc2 <- generateLearningCurveData(
  learners = lrn,
  task = datos.task,
  percs = seq(0.1, 1, by = 0.1),
  measures = list(ber, setAggregation(ber, train.mean)),
  resampling = rin2,
  show.info = FALSE
)
p <- plotLearningCurve(lc2, facet = "learner")
p
```

Observando la grafica de aprendizaje se puede observar que el modelo no tiene overfitting ya que aunque las curvas no son iguales, estas convergen.

#### 4.3.3 Predicciones y datos de validación

```{r}
train1$TIPO <- TIPO_temp
xgb_model <- xgboost(data = as.matrix(train1[, -which(colnames(train1) == "TIPO")]), 
                     label = train1[, c('TIPO')],
                     nrounds = 500, max_depth=4, eta=0.2254931, verbose=0)

# Make predictions
predictions <- predict(xgb_model, as.matrix(test1[, -which(colnames(test1) == "TIPO")]))

predictions <- ifelse(predictions > 0.5, 1, 0)
predictions <- factor(predictions)
# datos2 <- as.data.frame(datos2)
# datos2$TIPO <- factor(datos2$TIPO, levels = c('0', '1'))
# datos2$TIPO
confusion_matrix <- confusionMatrix(reference = as.factor(test1$TIPO), data = predictions)
confusion_matrix
```

```{r}
f1_score <- confusion_matrix$byClass["F1"]
f1_score
```

### 4.4 Random Forest

#### 4.4.1 Parameter tuning

```{r}
classifier <- makeClassifTask(
    data=train, 
    target="TIPO"
)

param_grid <- makeParamSet(makeDiscreteParam("ntree",values= seq(0, 500, by = 25)))

control_grid <- makeTuneControlGrid()

resample <- makeResampleDesc("CV", iters = 3L)

measure <- acc

set.seed(123)
dt_tuneparam <- tuneParams(learner='classif.randomForest', 
 task=classifier, 
 resampling = resample,
 measures = measure,
 par.set=param_grid, 
 control=control_grid, 
 show.info = TRUE)

result_hyperparam <- generateHyperParsEffectData(dt_tuneparam, partial.dep = TRUE)

best_parameters = setHyperPars(
    makeLearner("classif.randomForest"), 
    par.vals = dt_tuneparam$x
)

best_model = train(best_parameters, classifier)
best_params <- getHyperPars(best_model$learner)
best_params
```

Luego de realizar el tuneo de los parametros y ajustar los parametros de este modelo, se determinó que la mejor cantidad de árboles para nuestro conjunto de datos y objetivos específicos es 500. Esta elección se basó en un proceso de evaluación riguroso que consideró medidas de rendimiento y garantizó un equilibrio adecuado entre precisión y eficiencia computacional.

#### 4.4.2 Curva de aprendizaje

```{r warning=FALSE, message=FALSE}
datos.task = makeClassifTask(data = train, target = "TIPO")
rin2 = makeResampleDesc(method = "CV", iters = 10, predict = "both")
lrn = makeLearner("classif.randomForest", ntree = 500)
lc2 = generateLearningCurveData(learners = lrn, task = datos.task,
                                    percs = seq(0.1, 1, by = 0.1),
                                    measures = list(ber, setAggregation(ber, train.mean)), resampling = rin2,
                                    show.info = FALSE)
plotLearningCurve(lc2, facet = "learner")
```

Como se puede observar en la grafica que muestra la curva de aprendizaje el error de entrenamiento sigue disminuyendo a medida que se agregan más datos, pero el error de validación se estabiliza, sugiriendo asi que el modelo está memorizando los datos de entrenamiento y no generalizando bien a nuevos datos. Indicando así que el modelo presenta overfitting.

#### 4.4.3 Predicciones

```{r}
set.seed(123)

modelo_rf <- randomForest(TIPO ~ ., data = train, ntree = 500)


predicciones <- predict(modelo_rf, test)


confusion_matrix <- confusionMatrix(reference=test$TIPO, data = predicciones)
print(confusion_matrix)
```

```{r}
set.seed(123)
porcentaje <- 0.5
corte <- sample(nrow(test), nrow(test) * porcentaje)
test2 <- test[corte, ]
validation2 <- test[-corte,]

modelo_rf <- randomForest(TIPO ~ ., data = train, ntree = 500)


predicciones <- predict(modelo_rf, test2)
predicciones2 <- predict(modelo_rf, validation2)


confusion_matrix <- confusionMatrix(reference=test2$TIPO, data = predicciones)
confusion_matrix2 <- confusionMatrix(reference=validation2$TIPO, data = predicciones2)
print(confusion_matrix)
print(confusion_matrix)
```

Al usar solo la data de test y luego usar una de validation, se pude observar una diferencia importante en el accuracy. Esto indica overfitting.

```{r}
f1_score <- confusion_matrix$byClass["F1"]
f1_score
```

### 4.5 Regresión logística regularizada

#### 4.5.1 Parameter tunning

```{r}
grid <- expand.grid(alpha = seq(0, 1, by = 0.1), lambda = seq(0, 1, by = 0.1))
ctrl <- trainControl(method = "cv", number = 10)
model <- caret::train(TIPO ~ ., data = train, method = "glmnet", tuneGrid = grid, trControl = ctrl)

best_alpha <- model$bestTune$alpha
best_lambda <- model$bestTune$lambda
best_alpha
best_lambda
```

Luego de realizar el tuneo de parámetros se obtuvo que los mejores parámetros son un alpha de 1 y un lambda de 0.1.

#### 4.5.2 Curva de aprendizaje

```{r}
datos.task = makeClassifTask(data = train, target = "TIPO")
rin2 = makeResampleDesc(method = "CV", iters = 10, predict = "both")
lrn = makeLearner("classif.glmnet", alpha = 1, lambda = 0.1)
lc2 = generateLearningCurveData(learners = lrn, task = datos.task,
                                    percs = seq(0.1, 1, by = 0.1),
                                    measures = list(ber, setAggregation(ber, train.mean)), resampling = rin2,
                                    show.info = FALSE)
plotLearningCurve(lc2, facet = "learner")
```

Es posible observar, de la curva de aprendizaje, que a pesar de que las curvas están muy similares, el valor del accuracy con el 100% de los datos baja demasiado. Es posible que se tenga underfitting.

#### 4.5.3 Predicciones

```{r}
X_train <- as.matrix(select(train1, -TIPO))
y_train <- as.matrix(train1$TIPO)

X_test <- as.matrix(select(test1, -TIPO))
y_test <- as.matrix(test1$TIPO)

ridge_model <- glmnet(X_train, y_train, family = "binomial", alpha = 1, lambda = 0.1)

# Realizar predicciones en los datos de prueba
y_pred <- predict(ridge_model, newx = X_test, s = "lambda.min", type = "response")
y_pred <- ifelse(y_pred > 0.5, 1, 0)

cf <- confusionMatrix(reference = as.factor(test1$TIPO), data = as.factor(y_pred))
cf
```

```{r}
f1_score <- cf$byClass["F1"]
f1_score
```

## 5. Discusión de resultados

Tras haber realizado las predicciones con cada modelo obtuvimos los siguientes resultados:

### SVM

-   Accuracy: 0.9393
-   Sensitivity: 1
-   Specificity: 0.9103
-   F1: 0.9143

### Red Neuronal

-   Accuracy: 0.9391
-   Sensitivity: 0.9894
-   Specificity: 0.9148
-   F1: 0.9137

### XGBoost

-   Accuracy: 0.7924
-   Sensitivity: 0.836
-   Specificity: 0.7014
-   F1: 0.8449

### Random Forest

-   Accuracy: 0.9531
-   Sensitivity: 0.9687
-   Specificity: 0.9456
-   F1: 0.9303

### Regresión logística regularizada

-   Accuracy: 0.67
-   Sensitivity: 1
-   Specificity: 0
-   F1: 0.80697

Cabe mencionar que de los modelos anteriores los únicos que no estan ni sobre ajustados ni infra ajustado son: **La red neuronal y XGBoost**

Sobre la precisión de los diferentes modelos para poder predecir si el embarazo terminará siendo un aborto o si el embarazo se dara con normalidad, tenemos que el peor modelo fue la regresion logistica con un 0.67 de accuracy esto se pudo dar ya que este es un algoritmo de clasificación binaria por lo que la mejor metrica para evaluarlo no sería por medio del accuracy. Mientras que el mejor modelo fue el random forest con 0.95 de accuracy pero este es uno de los modelos que se encuentra sobre ajustado por lo que realmente no es un buen resultado para nuestra investigación, por la misma razón el modelo hace las predicciones con los patrones del dataset en especifico. Por otro lado de los modelos que **NO** se encuentran sobre ajustados tenemos que la red neuronal obtuvo un 0.93 de accuracy el cual fue superior al 0.79 de accuracy del XGBoost, por lo que el modelo que tuvo una precisión más alta y es significativo para la investigacion es **La red neuronal**.

Observando la capacidad de los modelos para predecir los verdaderos positivos (True Positive), se puede observar que el modelo que mejor predice los verdaderos positivos es la SVM y la regresion Logistica Regularizada, ambas con un valor de 1. Para la Regresion Logistica esto se puede deber a que su humbral este ajustado a los datos del data set y este marcando a todos los datos de una forma, agregado a desvalanceo que existe en los datos podria ser la razon del sobreajuste de este. Por otro lado tenemos tenemos que el XGBoost, con un valor de 0.836, esto se pudo dar ya que el modelo no logro capturar los patrones esenciales para casos positivos, o que el umbral utilizado para la clasificacion haya sido demasiado alto, lo que resulto en una menor tasa de verdaderos positivos. Por ultimo tenemos que el modelo que mejor pudo predecir los verdaderos positivos

Al observar la identificación de los verdaderos negativos (specificity) de los modelos, se puede evidenciar que el que mejor desempeño tuvo fue el random forest con 0.9456. El peor modelo para identificar verdaderos negativos fue la regresión logística regularizada, con 0. Esto último se puede deber al umbral utilizado para identificar una clase de la otra, de forma que la mayoría de datos tenían una probabilidad superior. Otro factor que afecta es el ligero desbalance mencionado previamente en los datos. Por otra parte, el random forest pudo tener un muy buen desempeño gracias a su manejo de características irrelevantes del dataset. Además, su mismo overfitting hizo que entendiera las reglas importantes del dataset que le permitieran identificar los verdaderos falsos.

Finalmente, la regresión logística tuvo el F1 más bajo y el random forest el más alto. Los atributos del random forest para el manejo de clases sin balance y su capacidad de aprender patrones complejos por su conjunto de árboles, permitieron que se desempeñara bastante bien en esta métrica. Por otra parte, la regresión logística puede que haya sido demasiado simple para la clasificación. Sin embargo, es importante notar que su F1 no es tan distinto a los demás modelos, a diferencia de su accuracy. Esto porque el desbalance de los datos causa que su accuracy no sea tan significativo. En general, por esta métrica la regresión no es un modelo tan malo, a pesar de su underfitting.

Tras haber hecho el análisis de cada uno de las métricas y tomando en cuenta únicamente los modelos que si proveen un aporte significativo a la investigación, el mejor modelo fue **La Red Neuronal**.
